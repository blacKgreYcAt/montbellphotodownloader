import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import re
import zipfile
import time
import random
from urllib.parse import urljoin, urlparse
import io

# --- é é¢è¨­å®šèˆ‡ iOS é¢¨æ ¼ CSS ---
st.set_page_config(page_title="Montbell ä¸‹è¼‰å™¨", page_icon="ğŸ”ï¸", layout="centered")

# iOS æ·±è‰²é¢¨æ ¼ + å¤§æŒ‰éˆ• CSS
st.markdown("""
<style>
    /* å¼·åˆ¶æ·±è‰²èƒŒæ™¯èˆ‡å­—é«” */
    .stApp {
        background-color: #000000;
        color: #FFFFFF;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    }
    
    /* è¼¸å…¥æ¡†åœ“è§’åŒ– */
    .stTextInput > div > div > input {
        border-radius: 12px;
        background-color: #1C1C1E;
        color: white;
        border: 1px solid #333;
    }
    
    /* æ•¸å­—è¼¸å…¥æ¡† */
    .stNumberInput > div > div > input {
        border-radius: 12px;
        background-color: #1C1C1E;
        color: white;
    }

    /* ä¸»è¦æŒ‰éˆ• (iOS Blue) - å¤§ Icon é¢¨æ ¼ */
    .stButton > button {
        width: 100%;
        height: 60px;
        border-radius: 14px;
        background-color: #0A84FF;
        color: white;
        font-weight: bold;
        font-size: 18px;
        border: none;
        transition: transform 0.1s;
    }
    .stButton > button:hover {
        background-color: #0070E0;
        transform: scale(0.98);
    }
    .stButton > button:active {
        background-color: #005BB5;
    }

    /* ä¸‹è¼‰æŒ‰éˆ• (iOS Green) */
    .stDownloadButton > button {
        width: 100%;
        height: 60px;
        border-radius: 14px;
        background-color: #30D158;
        color: black;
        font-weight: bold;
        font-size: 18px;
        border: none;
    }
    .stDownloadButton > button:hover {
        background-color: #28C14D;
    }

    /* é€²åº¦æ¢é¡è‰² */
    .stProgress > div > div > div > div {
        background-color: #0A84FF;
    }
    
    /* å¡ç‰‡å¼å®¹å™¨ */
    .css-1r6slb0 {
        background-color: #1C1C1E;
        border-radius: 16px;
        padding: 20px;
    }
</style>
""", unsafe_allow_html=True)

# --- æ ¸å¿ƒé‚è¼¯é¡åˆ¥ (æ”¹å¯«ç‚ºç„¡ç‹€æ…‹å‡½æ•¸) ---

def get_headers(referer=None):
    return {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': referer if referer else 'https://webshop.montbell.jp/',
        'Connection': 'keep-alive',
    }

def extract_images_from_html(soup, base_url):
    """å¾HTMLé é¢æå–åœ–ç‰‡URL"""
    image_urls = []
    
    # 1. fancy_largelink
    for link in soup.select('a.fancy_largelink'):
        hd_img_url = link.get('href')
        if hd_img_url:
            image_urls.append(urljoin(base_url, hd_img_url))
        
        img_tag = link.select_one('img')
        if img_tag and img_tag.get('src'):
            image_urls.append(urljoin(base_url, img_tag.get('src')))
            
    # 2. éš±è—å€åŸŸ
    for img in soup.select('#img_hidden_pre img, #img_hidden_later img'):
        if img.get('src'):
            image_urls.append(urljoin(base_url, img.get('src')))

    # 3. ä¸»åœ–
    main_img = soup.select_one('#largelinkImg')
    if main_img and main_img.get('src'):
        image_urls.append(urljoin(base_url, main_img.get('src')))

    # 4. ç¸®ç•¥åœ–èˆ‡å…¶é«˜è§£æç‰ˆæœ¬
    for img in soup.select('.cutImglArea img'):
        img_url = img.get('src')
        if img_url:
            full_url = urljoin(base_url, img_url)
            image_urls.append(full_url)
            # å˜—è©¦çŒœæ¸¬é«˜è§£æåº¦è·¯å¾‘
            if '/cut_c/' in full_url:
                image_urls.append(full_url.replace('/cut_c/', '/cut_k/').replace('cc_', 'ck_'))
            elif '/prod_c/' in full_url:
                image_urls.append(full_url.replace('/prod_c/', '/prod_k/').replace('c_', 'k_'))

    # å»é‡ä¸¦éæ¿¾
    return list(set(url for url in image_urls if url.startswith(('http', '//'))))

def extract_images_from_js(soup, base_url):
    """å¾JavaScriptæå–åœ–ç‰‡URL"""
    image_urls = []
    scripts = soup.find_all('script')
    
    image_data = {}
    image_paths = {}
    
    for script in scripts:
        script_text = script.string
        if script_text and ('cimages' in script_text or 'kimages' in script_text):
            for line in script_text.split('\n'):
                # æå–æª”å
                c_match = re.search(r"cimages\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if c_match:
                    k, v = c_match.groups()
                    image_data.setdefault(k, {})['cimage'] = v
                
                k_match = re.search(r"kimages\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if k_match:
                    k, v = k_match.groups()
                    image_data.setdefault(k, {})['kimage'] = v
                
                # æå–è·¯å¾‘
                cp_match = re.search(r"cimage_paths\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if cp_match:
                    k, v = cp_match.groups()
                    image_paths[f'cimage_paths_{k}'] = v
                
                kp_match = re.search(r"kimage_paths\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if kp_match:
                    k, v = kp_match.groups()
                    image_paths[f'kimage_paths_{k}'] = v

    for key, data in image_data.items():
        if 'cimage' in data:
            path = image_paths.get(f'cimage_paths_{key}', '/common/images/product/prod_c')
            image_urls.append(urljoin(base_url, f"{path}/{data['cimage']}"))
        if 'kimage' in data:
            path = image_paths.get(f'kimage_paths_{key}', '/common/images/product/prod_k')
            image_urls.append(urljoin(base_url, f"{path}/{data['kimage']}"))
            
    return list(set(image_urls))

# --- UI ä»‹é¢ ---

st.title("ğŸ”ï¸ Montbell åœ–ç‰‡ä¸‹è¼‰å™¨")
st.caption("Excel æ‰¹é‡ä¸‹è¼‰å·¥å…· | iOS Dark Mode Edition")

# 1. å´é‚Šæ¬„è¨­å®š
with st.expander("âš™ï¸ è¨­å®š (Settings)", expanded=False):
    domain = st.text_input("ç¶²ç«™åŸŸå", value="https://webshop.montbell.jp")
    delay = st.number_input("è«‹æ±‚å»¶é² (ç§’)", min_value=1, max_value=10, value=2)

# 2. æª”æ¡ˆä¸Šå‚³
uploaded_file = st.file_uploader("ğŸ“‚ è«‹ä¸Šå‚³ Excel æª”æ¡ˆ", type=['xlsx', 'xls'])

# ç‹€æ…‹è®Šæ•¸
if 'download_done' not in st.session_state:
    st.session_state.download_done = False
if 'zip_buffer' not in st.session_state:
    st.session_state.zip_buffer = None
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []

# 3. åŸ·è¡Œé‚è¼¯
if uploaded_file is not None:
    st.info(f"å·²è®€å–: {uploaded_file.name}")
    
    # è®€å– Excel é è¦½
    try:
        df = pd.read_excel(uploaded_file)
        st.dataframe(df.head(), height=150)
    except Exception as e:
        st.error(f"è®€å– Excel å¤±æ•—: {e}")
        st.stop()

    # é–‹å§‹ä¸‹è¼‰æŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œä¸‹è¼‰", key="start_btn"):
        st.session_state.log_messages = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        log_area = st.empty()
        
        # æº–å‚™è¨˜æ†¶é«” ZIP
        zip_buffer = io.BytesIO()
        
        try:
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                # æ¬„ä½è­˜åˆ¥
                model_col = next((c for c in df.columns if any(x in str(c).lower() for x in ['å‹è™Ÿ', 'model', 'id'])), df.columns[0])
                url_col = next((c for c in df.columns if any(x in str(c).lower() for x in ['ç¶²å€', 'url', 'link'])), None)
                
                total = len(df)
                downloaded_count = 0
                
                for idx, row in df.iterrows():
                    model_id = str(row[model_col]).strip()
                    if not model_id or model_id.lower() == 'nan': continue
                    
                    status_text.text(f"æ­£åœ¨è™•ç†: {model_id} ({idx+1}/{total})")
                    progress_bar.progress((idx + 1) / total)
                    
                    # æ±ºå®š URL
                    target_urls = []
                    if url_col and pd.notna(row[url_col]):
                        u = str(row[url_col]).strip()
                        if u.startswith('http'): target_urls.append(u)
                        
                    # æœå°‹æ¨¡å¼
                    if not target_urls:
                        search_url = f"{domain}/goods/list_search.php"
                        params = {'top_sk': model_id}
                        try:
                            time.sleep(delay)
                            resp = requests.get(search_url, params=params, headers=get_headers())
                            soup = BeautifulSoup(resp.content, 'html.parser')
                            for link in soup.find_all('a', href=True):
                                if 'goods/detail.php' in link['href']:
                                    target_urls.append(urljoin(search_url, link['href']))
                            
                            # è‹¥ç›´æ¥è·³è½‰
                            if not target_urls and 'goods/detail.php' in resp.url:
                                target_urls.append(resp.url)
                        except Exception as e:
                            st.session_state.log_messages.append(f"âŒ {model_id} æœå°‹éŒ¯èª¤: {e}")

                    # è™•ç†æ¯å€‹å•†å“é 
                    img_urls = []
                    for p_url in target_urls[:1]: # é™åˆ¶å–ç¬¬ä¸€å€‹åŒ¹é…å•†å“
                        try:
                            time.sleep(delay)
                            resp = requests.get(p_url, headers=get_headers())
                            s = BeautifulSoup(resp.content, 'html.parser')
                            img_urls.extend(extract_images_from_html(s, p_url))
                            img_urls.extend(extract_images_from_js(s, p_url))
                        except Exception as e:
                            pass
                    
                    img_urls = list(set(img_urls))
                    
                    if not img_urls:
                        st.session_state.log_messages.append(f"âš ï¸ {model_id}: æœªæ‰¾åˆ°åœ–ç‰‡")
                        continue

                    # ä¸‹è¼‰åœ–ç‰‡ä¸¦å¯«å…¥ ZIP
                    model_img_count = 0
                    for i, img_url in enumerate(img_urls):
                        try:
                            time.sleep(0.5)
                            img_resp = requests.get(img_url, headers=get_headers(p_url), timeout=10)
                            if img_resp.status_code == 200 and 'image' in img_resp.headers.get('Content-Type', ''):
                                # æª”åè™•ç†
                                parsed = urlparse(img_url)
                                fname = os.path.basename(parsed.path)
                                if not fname: fname = f"{model_id}_{i}.jpg"
                                
                                # å¯«å…¥ ZIP (è·¯å¾‘: å‹è™Ÿ/æª”å)
                                zip_file.writestr(f"{model_id}/{fname}", img_resp.content)
                                model_img_count += 1
                        except:
                            pass
                    
                    if model_img_count > 0:
                        downloaded_count += model_img_count
                        st.session_state.log_messages.append(f"âœ… {model_id}: ä¸‹è¼‰ {model_img_count} å¼µ")
                    
                    # é¡¯ç¤ºæœ€æ–°å¹¾ç­†æ—¥èªŒ
                    log_area.code("\n".join(st.session_state.log_messages[-5:]))

            # å®Œæˆè™•ç†
            st.session_state.zip_buffer = zip_buffer
            st.session_state.download_done = True
            st.success(f"ğŸ‰ è™•ç†å®Œæˆï¼å…±ä¸‹è¼‰ {downloaded_count} å¼µåœ–ç‰‡")
            
        except Exception as e:
            st.error(f"ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")

# 4. ä¸‹è¼‰æŒ‰éˆ• (è™•ç†å®Œæˆå¾Œå‡ºç¾)
if st.session_state.download_done and st.session_state.zip_buffer:
    st.markdown("---")
    st.write("### âœ… æª”æ¡ˆå·²æº–å‚™å¥½")
    
    # é‡ç½®æŒ‡é‡åˆ°é–‹é ­
    st.session_state.zip_buffer.seek(0)
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰åœ–ç‰‡å£“ç¸®æª” (ZIP)",
        data=st.session_state.zip_buffer,
        file_name="montbell_images.zip",
        mime="application/zip"
    )

    if st.button("æ¸…é™¤é‡ä¾†"):
        st.session_state.download_done = False
        st.session_state.zip_buffer = None
        st.experimental_rerun()