import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import re
import zipfile
import time
import math
import random
from urllib.parse import urljoin, urlparse
import io

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="Montbell ä¸‹è¼‰å™¨ (åŸç‰ˆæ ¸å¿ƒ)", page_icon="ğŸ”ï¸", layout="centered")

# --- iOS é¢¨æ ¼ CSS (åƒ…è¦–è¦ºï¼Œä¸å½±éŸ¿é‚è¼¯) ---
st.markdown("""
<style>
    .stApp { background-color: #000000; color: #FFFFFF; font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text", "Segoe UI", Roboto, sans-serif; }
    .stTextInput > div > div > input, .stNumberInput > div > div > input, .stSelectbox > div > div > div { border-radius: 12px; background-color: #1C1C1E; color: white; border: 1px solid #333; }
    div.stButton > button { width: 100%; aspect-ratio: 1 / 1; border-radius: 22px; background: linear-gradient(145deg, #0A84FF, #0070E0); color: white; font-weight: 600; font-size: 20px; border: none; box-shadow: 0 4px 15px rgba(0,0,0,0.3); margin-bottom: 10px; display: flex; flex-direction: column; justify-content: center; align-items: center; }
    div.stButton > button:hover { transform: scale(0.97); background: linear-gradient(145deg, #0070E0, #005BB5); }
    div.stDownloadButton > button { width: 100%; height: 60px; border-radius: 14px; background-color: #30D158; color: black; font-weight: bold; font-size: 18px; border: none; }
    div.stDownloadButton > button:hover { background-color: #28C14D; }
    .stProgress > div > div > div > div { background-color: #0A84FF; }
    .stExpander { background-color: #1C1C1E; border-radius: 16px; }
</style>
""", unsafe_allow_html=True)

# ==============================================================================
# æ ¸å¿ƒé‚è¼¯å€ - åš´æ ¼è¤‡è£½è‡ªåŸå§‹ Python è…³æœ¬
# ==============================================================================

def get_original_headers(referer=None):
    """å®Œå…¨é‚„åŸåŸå§‹è…³æœ¬çš„ Headers (ä½¿ç”¨é›»è…¦ç‰ˆ User-Agent)"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': referer if referer else 'https://webshop.montbell.jp/',
        'Connection': 'keep-alive',
    }

def original_extract_images_from_html(soup, base_url):
    """
    [åŸå§‹é‚è¼¯] å¾HTMLé é¢æå–åœ–ç‰‡URL
    è¤‡è£½è‡ª: extract_images_from_html æ–¹æ³•
    """
    image_urls = []
    
    # 1. å¾fancy_largelinkå…ƒç´ æå–åœ–ç‰‡
    fancy_links = soup.select('a.fancy_largelink')
    if fancy_links:
        for link in fancy_links:
            # é«˜è§£æåº¦åœ–ç‰‡ (href)
            hd_img_url = link.get('href')
            if hd_img_url:
                if not hd_img_url.startswith(('http://', 'https://')):
                    hd_img_url = urljoin(base_url, hd_img_url)
                image_urls.append(hd_img_url)
            
            # é é¢é¡¯ç¤ºåœ–ç‰‡ (img src)
            img_tag = link.select_one('img')
            if img_tag and img_tag.get('src'):
                img_url = img_tag.get('src')
                if not img_url.startswith(('http://', 'https://')):
                    img_url = urljoin(base_url, img_url)
                if img_url not in image_urls:
                    image_urls.append(img_url)
    
    # 2. å¾éš±è—å€åŸŸç²å–åœ–ç‰‡
    hidden_imgs = soup.select('#img_hidden_pre img, #img_hidden_later img')
    for img in hidden_imgs:
        img_url = img.get('src')
        if img_url:
            if not img_url.startswith(('http://', 'https://')):
                img_url = urljoin(base_url, img_url)
            if img_url not in image_urls:
                image_urls.append(img_url)
    
    # 3. ç²å–ä¸»åœ–
    main_img = soup.select_one('#largelinkImg')
    if main_img and main_img.get('src'):
        img_url = main_img.get('src')
        if not img_url.startswith(('http://', 'https://')):
            img_url = urljoin(base_url, img_url)
        if img_url not in image_urls:
            image_urls.append(img_url)
    
    # 4. å¾ç¸®ç•¥åœ–å€åŸŸç²å–åœ–ç‰‡
    thumb_imgs = soup.select('.cutImglArea img')
    for img in thumb_imgs:
        img_url = img.get('src')
        if img_url:
            if not img_url.startswith(('http://', 'https://')):
                img_url = urljoin(base_url, img_url)
            
            if img_url not in image_urls:
                image_urls.append(img_url)
            
            # å˜—è©¦ç²å–é«˜è§£æåº¦ç‰ˆæœ¬
            if '/cut_c/' in img_url:
                hd_img_url = img_url.replace('/cut_c/', '/cut_k/').replace('cc_', 'ck_')
                if hd_img_url not in image_urls:
                    image_urls.append(hd_img_url)
            elif '/prod_c/' in img_url:
                hd_img_url = img_url.replace('/prod_c/', '/prod_k/').replace('c_', 'k_')
                if hd_img_url not in image_urls:
                    image_urls.append(hd_img_url)
    
    # 5. å¾æ‰€æœ‰imgæ¨™ç±¤æå–åœ–ç‰‡ (é€™æ˜¯åŸç‰ˆé‚è¼¯çš„æœ€å¾Œä¸€æ­¥)
    all_images = soup.select('img[src]')
    for img in all_images:
        img_url = img.get('src')
        if img_url and any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):
            if not img_url.startswith(('http://', 'https://')):
                img_url = urljoin(base_url, img_url)
            if img_url not in image_urls:
                image_urls.append(img_url)
                
    return image_urls

def original_extract_images_from_js(soup, base_url):
    """
    [åŸå§‹é‚è¼¯] å¾JavaScriptæå–åœ–ç‰‡URL
    è¤‡è£½è‡ª: extract_images_from_js æ–¹æ³•
    """
    image_urls = []
    scripts = soup.find_all('script')
    
    image_data = {}
    image_paths = {}
    
    for script in scripts:
        script_text = script.string
        if script_text and ('cimages' in script_text or 'kimages' in script_text):
            # æå–åœ–ç‰‡æ–‡ä»¶åå’Œè·¯å¾‘
            for line in script_text.split('\n'):
                # åœ–ç‰‡æ–‡ä»¶å
                cimages_match = re.search(r"cimages\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if cimages_match:
                    key, value = cimages_match.groups()
                    if key not in image_data:
                        image_data[key] = {}
                    image_data[key]['cimage'] = value
                
                kimages_match = re.search(r"kimages\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if kimages_match:
                    key, value = kimages_match.groups()
                    if key not in image_data:
                        image_data[key] = {}
                    image_data[key]['kimage'] = value
                
                # åœ–ç‰‡è·¯å¾‘
                cimage_path_match = re.search(r"cimage_paths\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if cimage_path_match:
                    key, value = cimage_path_match.groups()
                    image_paths[f'cimage_paths_{key}'] = value
                
                kimage_path_match = re.search(r"kimage_paths\['([^']+)'\]\s*=\s*'([^']+)'", line)
                if kimage_path_match:
                    key, value = kimage_path_match.groups()
                    image_paths[f'kimage_paths_{key}'] = value
    
    # æ§‹å»ºå®Œæ•´URL
    if image_data:
        for key, data in image_data.items():
            # é«˜è§£æåº¦åœ–ç‰‡ (cimage)
            if 'cimage' in data:
                cimage_path = image_paths.get(f'cimage_paths_{key}', '/common/images/product/prod_c')
                cimage_url = f"{base_url}{cimage_path}/{data['cimage']}"
                # ä¿®æ­£ï¼šåŸå§‹è…³æœ¬é€™è£¡å¯èƒ½æ²’æœ‰åš urljoinï¼Œä½†ç‚ºäº†ä¿éšªèµ·è¦‹æˆ‘å€‘åšä¸€ä¸‹è™•ç†ï¼Œå¦‚æœåŸè…³æœ¬ä¾è³´å­—ä¸²æ‹¼æ¥å‰‡ä¿æŒ
                # ç‚ºäº†é¿å…é›™é‡ slashï¼Œé€™è£¡ç°¡å–®è™•ç†
                cimage_url = cimage_url.replace('https://webshop.montbell.jp//', 'https://webshop.montbell.jp/') 
                image_urls.append(cimage_url)
            
            # ä½è§£æåº¦åœ–ç‰‡ (kimage)
            if 'kimage' in data:
                kimage_path = image_paths.get(f'kimage_paths_{key}', '/common/images/product/prod_k')
                kimage_url = f"{base_url}{kimage_path}/{data['kimage']}"
                kimage_url = kimage_url.replace('https://webshop.montbell.jp//', 'https://webshop.montbell.jp/')
                image_urls.append(kimage_url)
                
    return image_urls

def extract_color_code(filename):
    """è¼”åŠ©åŠŸèƒ½ï¼šæå–é¡è‰²ä»£ç¢¼ (é€™æ˜¯Web Appæ–°å¢çš„å¯¦ç”¨åŠŸèƒ½ï¼Œä¿ç•™)"""
    try:
        name_without_ext = os.path.splitext(filename)[0]
        if '_' in name_without_ext:
            parts = name_without_ext.split('_')
            last_part = parts[-1]
            if last_part.isdigit() and len(parts) > 1: return parts[-2]
            return last_part
    except: pass
    return None

# ==============================================================================
# UI ä»‹é¢èˆ‡ä¸»æµç¨‹
# ==============================================================================

st.title("ğŸ”ï¸ Montbell ä¸‹è¼‰å™¨ (åŸç‰ˆæ ¸å¿ƒ)")
st.caption("v2.0 åš´æ ¼å¾©åˆ»åŸå§‹ Python é‚è¼¯ | iOS Style GUI")

uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šå‚³ Excel (å«å‹è™Ÿæ¬„ä½)", type=['xlsx', 'xls'])

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        
        # æ¬„ä½åµæ¸¬
        model_col = next((c for c in df.columns if any(x in str(c).lower() for x in ['å‹è™Ÿ', 'model', 'id'])), df.columns[0])
        url_col = next((c for c in df.columns if any(x in str(c).lower() for x in ['ç¶²å€', 'url', 'link'])), None)
        
        total_items = len(df)
        BATCH_SIZE = 50
        total_batches = math.ceil(total_items / BATCH_SIZE)

        st.write("---")
        
        # åˆ†æ‰¹é¸æ“‡å™¨
        col1, col2 = st.columns([2, 1])
        with col1:
            batch_options = [f"ğŸ“¦ ç¬¬ {i+1} æ‰¹ (å‹è™Ÿ {i*BATCH_SIZE+1} - {min((i+1)*BATCH_SIZE, total_items)})" for i in range(total_batches)]
            selected_batch_str = st.selectbox("é¸æ“‡æ‰¹æ¬¡", batch_options)
            try:
                batch_number = int(re.search(r'\d+', selected_batch_str).group())
                batch_index = batch_number - 1
            except: batch_index = 0
            start_idx = batch_index * BATCH_SIZE
            end_idx = min((batch_index + 1) * BATCH_SIZE, total_items)
            batch_df = df.iloc[start_idx:end_idx]
            
        with col2:
            st.metric("æœ¬æ‰¹æ•¸é‡", f"{len(batch_df)}")

        with st.expander("âš™ï¸ é€²éšè¨­å®š"):
            domain = st.text_input("åŸŸå", "https://webshop.montbell.jp")
            delay = st.number_input("å»¶é²(ç§’)", 1, 10, 2)

        st.write("---")

        # åŸ·è¡ŒæŒ‰éˆ•
        b_col1, b_col2, b_col3 = st.columns([1, 2, 1])
        start_process = False
        with b_col2:
            if st.button(f"ğŸš€\né–‹å§‹ä¸‹è¼‰\næœ¬æ‰¹æ¬¡", key="run_batch"):
                start_process = True

        if start_process:
            progress_bar = st.progress(0)
            status_text = st.empty()
            log_area = st.empty()
            logs = []
            report_data = []
            zip_buffer = io.BytesIO()
            download_count = 0
            
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                
                # === é–‹å§‹åŸå§‹è…³æœ¬çš„è¿­ä»£é‚è¼¯ ===
                for i, (orig_idx, row) in enumerate(batch_df.iterrows()):
                    model_number = str(row[model_col]).strip()
                    if not model_number or model_number == 'nan': continue
                    
                    # é€²åº¦æ›´æ–°
                    progress = (i + 1) / len(batch_df)
                    progress_bar.progress(progress)
                    status_text.text(f"æ­£åœ¨è™•ç†: {model_number}")
                    
                    product_links = []
                    search_url = None

                    # 1. å˜—è©¦å¾ Excel ç²å– URL
                    if url_col and pd.notna(row[url_col]):
                        product_url = str(row[url_col]).strip()
                        if product_url and product_url.lower() != 'nan':
                            if not product_url.startswith(('http://', 'https://')):
                                product_url = urljoin(domain, product_url)
                            product_links.append(product_url)
                            search_url = domain
                    
                    # 2. å¦‚æœæ²’æœ‰ URLï¼ŒåŸ·è¡Œæœå°‹ (åŸå§‹é‚è¼¯)
                    if not product_links:
                        search_url = f"{domain}/goods/list_search.php"
                        params = {'top_sk': model_number}
                        
                        try:
                            time.sleep(delay)
                            # ä½¿ç”¨åŸå§‹ Headers
                            resp = requests.get(search_url, params=params, headers=get_original_headers())
                            
                            # è§£æé é¢ (åŸå§‹é‚è¼¯ï¼šåŒæ™‚æª¢æŸ¥ detail.php å’Œ disp.php)
                            soup = BeautifulSoup(resp.content, 'html.parser')
                            for link in soup.find_all('a', href=True):
                                href = link.get('href', '')
                                if 'goods/detail.php' in href or 'goods/disp.php' in href:
                                    product_links.append(urljoin(search_url, href))
                            
                            # å¦‚æœæ²’æ‰¾åˆ°é€£çµä½†é é¢æœ¬èº«å°±æ˜¯å•†å“é  (è·³è½‰)
                            if not product_links and ('goods/detail.php' in resp.url or 'goods/disp.php' in resp.url):
                                product_links.append(resp.url)
                        except Exception as e:
                            logs.append(f"âŒ {model_number} æœå°‹å¤±æ•—: {e}")

                    # 3. è™•ç†å•†å“é é¢
                    relevant_images = []
                    # åŸå§‹è…³æœ¬é€™è£¡åªå–å‰ 3 å€‹é€£çµ
                    for product_url in product_links[:3]:
                        try:
                            time.sleep(delay)
                            product_response = requests.get(product_url, headers=get_original_headers(search_url))
                            
                            if product_response.status_code != 200: continue
                            
                            product_soup = BeautifulSoup(product_response.content, 'html.parser')
                            
                            # === èª¿ç”¨åŸå§‹æå–å‡½æ•¸ ===
                            html_images = original_extract_images_from_html(product_soup, product_url)
                            if html_images: relevant_images.extend(html_images)
                            
                            js_images = original_extract_images_from_js(product_soup, product_url)
                            if js_images: relevant_images.extend(js_images)
                            
                        except Exception as e:
                            pass
                    
                    # å»é™¤é‡è¤‡URL
                    relevant_images = list(set(relevant_images))
                    
                    # 4. ä¸‹è¼‰åœ–ç‰‡ (åŸå§‹é‚è¼¯)
                    item_img_count = 0
                    item_colors = set()
                    
                    for img_idx, img_url in enumerate(relevant_images):
                        try:
                            time.sleep(delay / 2) # åŸå§‹è…³æœ¬é€™è£¡æœ‰éš¨æ©Ÿå»¶é²ï¼ŒWebç‰ˆç¨å¾®å›ºå®šä¸€é»
                            
                            headers = get_original_headers(product_url) # ä½¿ç”¨åŸå§‹ User-Agent
                            headers['Accept'] = 'image/webp,image/apng,image/*,*/*;q=0.8'
                            
                            # å…ˆ HEAD æª¢æŸ¥ (åŸå§‹é‚è¼¯)
                            try:
                                head_response = requests.head(img_url, headers=headers, timeout=5)
                            except: continue # å¦‚æœ head å¤±æ•—å°±è·³é
                            
                            if head_response.status_code == 200:
                                img_response = requests.get(img_url, headers=headers, stream=True)
                                content_type = img_response.headers.get('Content-Type', '')
                                
                                if 'image/' in content_type:
                                    parsed_url = urlparse(img_url)
                                    original_filename = os.path.basename(parsed_url.path)
                                    
                                    # æª”åè™•ç†
                                    if not original_filename:
                                        ext = '.' + content_type.split('/')[-1]
                                        original_filename = f"{model_number}_{img_idx+1}{ext}"
                                    
                                    original_filename = original_filename.split('?')[0]
                                    
                                    # å¯«å…¥ ZIP
                                    # ç‚ºäº†é¿å…é‡åè¦†è“‹ï¼Œé€™è£¡åšç°¡å–®çš„ unique è™•ç†
                                    zip_path = f"{model_number}/{original_filename}"
                                    if zip_path in zf.namelist():
                                        name, ext = os.path.splitext(original_filename)
                                        zip_path = f"{model_number}/{name}_{img_idx}{ext}"
                                        
                                    zf.writestr(zip_path, img_response.content)
                                    item_img_count += 1
                                    
                                    # æ”¶é›†é¡è‰² (å ±è¡¨ç”¨)
                                    c = extract_color_code(original_filename)
                                    if c: item_colors.add(c)
                        except: pass
                    
                    # è¨˜éŒ„æ—¥èªŒ
                    colors_str = ",".join(sorted(list(item_colors))) if item_colors else "ç„¡/æœªè­˜åˆ¥"
                    report_data.append({
                        "å•†å“å‹è™Ÿ": model_number,
                        "åœ–ç‰‡æ•¸é‡": item_img_count,
                        "å·²å–å¾—é¡è‰²": colors_str,
                        "ç‹€æ…‹": "æˆåŠŸ" if item_img_count > 0 else "å¤±æ•—/ç„¡åœ–ç‰‡"
                    })
                    
                    if item_img_count > 0:
                        download_count += item_img_count
                        logs.append(f"âœ… {model_number}: {item_img_count} å¼µ ({colors_str})")
                    else:
                        logs.append(f"âš ï¸ {model_number}: ç„¡åœ–ç‰‡")
                    log_area.code("\n".join(logs[-3:]))

                # ç”Ÿæˆ Excel
                if report_data:
                    df_report = pd.DataFrame(report_data)
                    with io.BytesIO() as excel_buffer:
                        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                            df_report.to_excel(writer, index=False, sheet_name='ä¸‹è¼‰æ‘˜è¦')
                        zf.writestr(f"å ±è¡¨_ç¬¬{batch_index+1}æ‰¹.xlsx", excel_buffer.getvalue())

            status_text.text("âœ… æœ¬æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
            progress_bar.progress(100)
            zip_buffer.seek(0)
            
            st.success(f"ğŸ‰ æˆåŠŸæ‰“åŒ… {download_count} å¼µåœ–ç‰‡")
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è¼‰ç¬¬ {batch_index+1} æ‰¹å£“ç¸®æª”",
                data=zip_buffer,
                file_name=f"montbell_batch_{batch_index+1}_original_logic.zip",
                mime="application/zip"
            )

    except Exception as e:
        st.error(f"åŸ·è¡ŒéŒ¯èª¤: {e}")